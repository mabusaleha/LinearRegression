{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74d916c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary Python libraries\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "sns.set(style=\"white\")\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import statsmodels.api as sm\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91005a07",
   "metadata": {},
   "source": [
    "### Step 1:  EDA Process starts -  Inspecting the Dataframe for understanding provided data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870f822f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"E:/IIITB_Upgrad_AI_ML_Course/BikeLinearRegression/day.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f099db6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking size of the data\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eaebf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking data types of columns and null value analysis if any\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdaa80f",
   "metadata": {},
   "source": [
    "##### Observation : All data points are non-null and therefore do not require any imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79891f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy of original dataframe for duplicate check\n",
    "df_dup_check = df\n",
    "\n",
    "# Checking for duplicates and dropping the entire duplicate row if any\n",
    "df_dup_check.drop_duplicates(subset=None, inplace=True)\n",
    "df_dup_check.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb0b2d4",
   "metadata": {},
   "source": [
    "##### Observation: The shape after running the drop duplicate command is same as the original dataframe. Hence we can conclude that there were not any duplicate values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92024b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the spread of numerical columns\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4d78e7",
   "metadata": {},
   "source": [
    "#### Performing numerical and categorical analysis on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ad988e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair Plot for numerical variables\n",
    "\n",
    "vars=[\"cnt\",\"temp\",\"atemp\",\"hum\",\"windspeed\",\"casual\",\"registered\"]\n",
    "sns.pairplot(df[vars])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145ba2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box Plots for categorical variables\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.subplot(3,3,1)\n",
    "sns.boxplot(x='season',y='cnt',data=df)\n",
    "plt.subplot(3,3,2)\n",
    "sns.boxplot(x='yr',y='cnt',data=df)\n",
    "plt.subplot(3,3,3)\n",
    "sns.boxplot(x='mnth',y='cnt',data=df)\n",
    "plt.subplot(3,3,4)\n",
    "sns.boxplot(x='holiday',y='cnt',data=df)\n",
    "plt.subplot(3,3,5)\n",
    "sns.boxplot(x='weekday',y='cnt',data=df)\n",
    "plt.subplot(3,3,6)\n",
    "sns.boxplot(x='weathersit',y='cnt',data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2999ac0",
   "metadata": {},
   "source": [
    "#### Observations from EDA of categorical and numerical variables:\n",
    "###### Season - We can notice a positive trend in the number of customers in 2 - Summer, 3 - Fall and 4 - Winter seasons\n",
    "###### Year - The overall business shows a increasing trend in their user base year on year\n",
    "###### Month - Similar to the season trend, there is a postive trend in the months of summer, fall and winter.\n",
    "###### Holiday : On holidays, the users show a wider spread in the counts. On normal days, the users are more than holidays\n",
    "###### Weekday : Weekdays or weekends do not show any specific trend here.\n",
    "###### Weathersit : Clearer weathers show a postive trend in the number of bike users\n",
    "- 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "- 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "- 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707715dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing corelations among the variables using a heatmap\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "sns.heatmap(df.corr(),annot=True,cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1311d80a",
   "metadata": {},
   "source": [
    "#### EDA Conlclusions:\n",
    "\n",
    "###### Based on the high level analysis of the data and the data dictionary, the following variables can be removed for further analysis:\n",
    "###### instant: It is only an index value\n",
    "###### dteday: This has the date, Since we already have separate columns for 'year' & 'month' we could live without this column\n",
    "###### casual : Count of bike booked by different categories of customers. \n",
    "\n",
    "###### From the pairplot as well as the correlation heatmap, we can concur that total bike rental value 'cnt = 'casual' + 'registered'. Since our objective is to find the total count of bikes and not by specific category, we will ignore these two columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc765c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the unwanted columns\n",
    "df.drop(['instant','dteday','casual'],axis=1,inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88871e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing corelations among the variables using a heatmap\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "sns.heatmap(df.corr(),annot=True,cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d594fd",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "###### Before continue further by satistical significance, our top 3 independent variables explaining changes of bike demands are :\n",
    "\n",
    "##### 1st : temp/atemp/instant (+0.63)\n",
    "##### 2nd : yr (+0.57)\n",
    "##### 3rd : seasosn (+0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a69819",
   "metadata": {},
   "source": [
    "### Step 2 : Data Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203cddd4",
   "metadata": {},
   "source": [
    "#### Convert categorical variables into dummy variables\n",
    "\n",
    "#### List of categorical variables:\n",
    "\n",
    "#### season : 1:spring, 2:summer, 3:fall, 4:winter\n",
    "#### yr: 0: 2018, 1:2019\n",
    "#### months : 1 to 12\n",
    "#### holiday : 0 and 1\n",
    "#### workingday : 0 and 1\n",
    "#### weekday : 1 to 6\n",
    "#### weathersit : 1, 2, 3 and 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba88c9a",
   "metadata": {},
   "source": [
    "### Observation:\n",
    "#### From the list of columns we do not need to convert yr, holiday and workingday since they are already in a binary format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cd2555",
   "metadata": {},
   "source": [
    "#### Converting season into dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb521c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing season column codes with their descriptions\n",
    "\n",
    "df.season=df.season.map({1:'spring', 2:'summer', 3:'fall', 4:'winter'})\n",
    "df.season.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48b0e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting dummy variable for season variable\n",
    "\n",
    "season=pd.get_dummies(df.season, drop_first=True) #Dropping the first dummy variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b455e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking dummy variable for season\n",
    "\n",
    "season.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc91923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatinating season to the original dataframe\n",
    "\n",
    "df=pd.concat([df,season], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125dd6fa",
   "metadata": {},
   "source": [
    "#### Converting weathersit into dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573e73e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking weathersit column codes with their descriptions\n",
    "\n",
    "df.weathersit.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66ef29d",
   "metadata": {},
   "source": [
    "#### We do not have any data points for type 4 weather. We can create only two dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca5a434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputing weathersit column codes with their descriptions\n",
    "\n",
    "df.weathersit=df.weathersit.map({1:'clear', 2:'misty', 3:'cloudy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a4cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking dummy variable for weathersit\n",
    "\n",
    "df.weathersit.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228b1d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting dummy variable for weathersit variable\n",
    "\n",
    "weathersit=pd.get_dummies(df.weathersit, drop_first=True) # Dropping the first dummy variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e538b75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking dummy variable for weathersit\n",
    "\n",
    "weathersit.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33694e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatinating weathersit to the original dataframe\n",
    "\n",
    "df=pd.concat([df,weathersit], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f919cc4",
   "metadata": {},
   "source": [
    "##### Converting month into dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e02df61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputing month column codes with their descriptions\n",
    "\n",
    "df.mnth=df.mnth.map({1:'jan', 2:'feb', 3:'mar',4:'apr',5:'may',6:'jun',\n",
    "                     7:'jul',8:'aug',9:'sep',10:'oct',11:'nov',12:'dec'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313196f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mnth.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f37797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting dummy variable for month variable\n",
    "\n",
    "mnth=pd.get_dummies(df.mnth, drop_first=True) # Dropping the first dummy variable\n",
    "mnth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9743547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatinating mnth to the original dataframe\n",
    "\n",
    "df=pd.concat([df,mnth], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6af67bd",
   "metadata": {},
   "source": [
    "#### Converting weekday into dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3e90fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputing weekday column codes with their descriptions\n",
    "\n",
    "df.weekday=df.weekday.map({0:'mon', 1:'tues', 2:'wed',3:'thurs',4:'fri',5:'sat',6:'sun'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a06978",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.weekday.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcedc64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting dummy variable for weekday variable\n",
    "\n",
    "weekday=pd.get_dummies(df.weekday, drop_first=True)\n",
    "weekday.head()\n",
    "#We would not need all the 7 days here, we will drop off one day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c186ddc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatinating mnth to the original dataframe\n",
    "df=pd.concat([df,weekday], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36105087",
   "metadata": {},
   "source": [
    "#### We have now converted all dummy variables for all categorical variables in the data. Let's drop the converted categorical variables from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b0e920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping season and weathersit\n",
    "df.drop(columns=['season','weathersit','mnth','weekday'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e904b6",
   "metadata": {},
   "source": [
    "#### We will convert registered users into categorical variables by performing binning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd97e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the data distribution of registered users\n",
    "\n",
    "plt.hist(df.registered,bins=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2db8497",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking quantiles of registered variables in 5 bins\n",
    "\n",
    "df.registered.quantile([0.2, 0.4, 0.6, 0.8, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1355c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the names of the bins\n",
    "\n",
    "bins=['very low','low','medium','high','very high']\n",
    "df['registered_bin']=pd.qcut(df['registered'],q=[0, 0.2, 0.4, 0.6, 0.8, 1], labels=bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeb9b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the spread of data in the bins\n",
    "\n",
    "df['registered_bin'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33006c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting dummy variable for registered_bin variable\n",
    "\n",
    "registered_bin=pd.get_dummies(df.registered_bin, drop_first=True) #Dropping the first dummy variable\n",
    "registered_bin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858bb01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatinating bins to the original dataframe\n",
    "\n",
    "df=pd.concat([df,registered_bin], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e604d5d",
   "metadata": {},
   "source": [
    "##### Dropping other similar variables like date and instant from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e76870",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping atemp since we have a similar variable temp in the data\n",
    "\n",
    "df.drop(columns=['atemp'], inplace=True)\n",
    "\n",
    "#Dropping causal and registered since we can have only one target in the data\n",
    "\n",
    "df.drop(columns=['registered','registered_bin'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d630410a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the fields in the dataset after data preparation\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b545f8",
   "metadata": {},
   "source": [
    "#### Step 3: Splitting the data into test and train datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d8cc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70584b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, train_size=0.7, test_size=0.3, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7dc4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shape of train dataset\n",
    "\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc740de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4633b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shape of test  dataset\n",
    "\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114c01ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da01a64",
   "metadata": {},
   "source": [
    "### Observation:\n",
    "####  Based on the 70% - 30% split between train and test dataset we have 510 rows in train dataset and 219 in test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985b221e",
   "metadata": {},
   "source": [
    "#### Step 4: Resclaing the feature variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5b30d6",
   "metadata": {},
   "source": [
    "#### From the data, we can see temp, hum and windspeed have larger and decimal values compared to others. We can normalized the numbers using the MinMax method and have all the numbers within 0 and 1 rang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d91b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4b45fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying fit_transform to normalize temp, atemp, hum and windspeed numerical columns\n",
    "\n",
    "rescalevar=['temp','hum','windspeed','cnt']\n",
    "df_train[rescalevar]=scaler.fit_transform(df_train[rescalevar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a007ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking minimum and maximum values of the normalized variables\n",
    "\n",
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e020d0b1",
   "metadata": {},
   "source": [
    "#### Step 5: Checking linearity and corealtion in the train dataset after rescaling and dummy field conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7247dbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a heatmap to check linearity\n",
    "\n",
    "plt.figure(figsize = (30, 20))\n",
    "sns.heatmap(df_train.corr(), annot = True, cmap=\"YlGnBu\",annot_kws={\"fontsize\":14})\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4038bc",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "##### We can see that temperature has the most corelated to users than any other variable with 0.64 coefficient. We will proceed to try a regression model using temp as our predictor variable. We will not use registered bin variables yet and use them as an experimental addition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d71035",
   "metadata": {},
   "source": [
    "#### Step 6: Building our MLR model\n",
    "##### Model 1: Using a single selected variable - temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24975bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statmodels for our MLR\n",
    "\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1ea786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our feature and target variable datasets in y_train and X_train\n",
    "\n",
    "y_train=df_train.pop('cnt')\n",
    "X_train=df_train[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb49a061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a constant to X_train\n",
    "\n",
    "X_train_lm=sm.add_constant(X_train['temp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771d6ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a LR object which we will use to fit the line\n",
    "\n",
    "lr1 = sm.OLS(y_train, X_train_lm).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb366a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the summary\n",
    "\n",
    "lr1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4675931",
   "metadata": {},
   "source": [
    "#### Observation:We have R squared value of 0.41 with just the temperature variable. Meaning 41% of the variance is explained by temperature feature. P-value of the feature is also 0. So, temperature is statistically significant here.\n",
    "\n",
    "#### Our line beta 0 is 0.6209 and beta 1 is 0.1668"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bde9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting our regression line through the target variable\n",
    "\n",
    "plt.scatter(X_train_lm.iloc[:, 1], y_train)\n",
    "plt.plot(X_train_lm.iloc[:, 1], 0.1668 + 0.6209*X_train_lm.iloc[:, 1], 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c118c1",
   "metadata": {},
   "source": [
    "#### Step 7 : Using RFE to select variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171d06e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries for RFE\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a17d79a",
   "metadata": {},
   "source": [
    "##### Since we would be using registered bins as an experimental feature addition, we will exclude those features for RFE selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a022ad33",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(columns=['low','medium','high','very high'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d259e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a RFE object\n",
    "\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "#rfe = RFE(lm,10)\n",
    "rfe = RFE(lm, n_features_to_select=15)\n",
    "rfe = rfe.fit(X_train, y_train) #fitting the object on our train datase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164fcd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result of Recursive elimination of variables and their rankings\n",
    "\n",
    "list(zip(X_train.columns,rfe.support_,rfe.ranking_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f5f0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing variables with significant weights in a variable\n",
    "\n",
    "rfe_vars=X_train.columns[rfe.support_]\n",
    "rfe_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f899111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking which columns have been eleminated \n",
    "X_train.columns[~rfe.support_]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd64524",
   "metadata": {},
   "source": [
    "##### Model 2: Building the model using RFE selected variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e83d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a train dataset using RFE variables\n",
    "\n",
    "X_train_rfe=X_train[rfe_vars]\n",
    "X_train_rfe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4244e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a constant to X_train_rfe\n",
    "\n",
    "X_train_rfe=sm.add_constant(X_train_rfe)\n",
    "X_train_rfe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa48ab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a LR object which we will use to fit the line.\n",
    "\n",
    "lr2 = sm.OLS(y_train, X_train_rfe).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cceae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the summary\n",
    "\n",
    "lr2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7d9e50",
   "metadata": {},
   "source": [
    "#### Observation: From the p-values, we can see that we have variables with high p - values or variables which are not significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773e5c7d",
   "metadata": {},
   "source": [
    "#### Calculating VIF for model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b99b9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the VIFs for the new model\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "X_train_rfe = X_train_rfe.drop(['const'], axis=1) # Dropping constant variable from the df\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "X = X_train_rfe\n",
    "vif['Features'] = X.columns\n",
    "vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa92675",
   "metadata": {},
   "source": [
    "##### Dropping insignificant variables and running the model.\n",
    "##### As we can see hum have high p-value. Such variable(s) are insignificant and should be dropped.\n",
    "\n",
    "##### We will start with dropping a single variable and recalculate its impact on other variables. Since hum has the highest p-value and and VIF>5. We will drop this variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd07197d",
   "metadata": {},
   "source": [
    "##### Model 3: Building the model after dropping hum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd89544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping hum variable\n",
    "X_train_rfe.drop(columns='hum', inplace=True)\n",
    "\n",
    "# Adding a constant to X_train_rfe\n",
    "X_train_rfe=sm.add_constant(X_train_rfe)\n",
    "X_train_lm3=sm.add_constant(X_train_rfe)\n",
    "\n",
    "# Creating a LR object which we will use to fit the line.\n",
    "lr3 = sm.OLS(y_train, X_train_rfe).fit()\n",
    "\n",
    "#Checking the summary\n",
    "lr3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80fdcd7",
   "metadata": {},
   "source": [
    "#### Recalculating VIF for model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee6b9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recalculating VIF\n",
    "X_train_rfe = X_train_rfe.drop(['const'], axis=1) # Dropping constant variable from the df\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "X = X_train_rfe\n",
    "vif['Features'] = X.columns\n",
    "vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb689aa",
   "metadata": {},
   "source": [
    "##### Observation: We have all of our variables within VIF 5 and almost zero p-values coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c345873",
   "metadata": {},
   "source": [
    "##### At this stage, we have an adjusted R-sqaure values 0.84 which means our model explains 84% of the variance in the train data.\n",
    "##### Let us try to manually add some significant variables to check if it improves our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d34346",
   "metadata": {},
   "outputs": [],
   "source": [
    "#listing columns which have been used in lr3\n",
    "\n",
    "X_train_rfe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c68723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# listing columns which can be explored and added to the model\n",
    "\n",
    "X_train.columns.difference(X_train_rfe.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11749e9f",
   "metadata": {},
   "source": [
    "#### June has a co-relation of 0.22 with the count variable. Let's try adding june and recalculating the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1946b2",
   "metadata": {},
   "source": [
    "#### Model 4: Building the model after adding june."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e753272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Jun variable\n",
    "X_train_rfe['jun']=X_train['jun']\n",
    "\n",
    "# Adding a constant to X_train_rfe\n",
    "X_train_rfe=sm.add_constant(X_train_rfe)\n",
    "\n",
    "# Creating a LR object which we will use to fit the line.\n",
    "lr4 = sm.OLS(y_train, X_train_rfe).fit()\n",
    "\n",
    "#Checking the summary\n",
    "lr4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5642509e",
   "metadata": {},
   "source": [
    "#### The p-value increased after adding june. We should drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2769accb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rfe.drop(columns='jun', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039905c3",
   "metadata": {},
   "source": [
    "#### Feb has a negative co-relation of 0.27 with the count variable. Let's try adding february and recalculating the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f695d7e2",
   "metadata": {},
   "source": [
    "#### Model 5: Building the model after adding feb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3528de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Feb variable and building the model.\n",
    "X_train_rfe['feb']=X_train['feb']\n",
    "\n",
    "# Adding a constant to X_train_rfe\n",
    "X_train_rfe=sm.add_constant(X_train_rfe)\n",
    "\n",
    "# Creating a LR object which we will use to fit the line.\n",
    "lr5 = sm.OLS(y_train, X_train_rfe).fit()\n",
    "\n",
    "#Checking the summary\n",
    "lr5.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081f410b",
   "metadata": {},
   "source": [
    "### The p-value increased after adding feb. We should drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cbdca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rfe.drop(columns='feb', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e511e4",
   "metadata": {},
   "source": [
    "#### Model 6: Building the model after adding march."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a71407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding mar variable and building the model.\n",
    "X_train_rfe['mar']=X_train['mar']\n",
    "\n",
    "# Adding a constant to X_train_rfe\n",
    "X_train_rfe=sm.add_constant(X_train_rfe)\n",
    "\n",
    "# Creating a LR object which we will use to fit the line.\n",
    "lr6 = sm.OLS(y_train, X_train_rfe).fit()\n",
    "\n",
    "#Checking the summary\n",
    "lr6.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a60e3e",
   "metadata": {},
   "source": [
    "### The p-value increased after adding mar. We should drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a7b689",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rfe.drop(columns='mar', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5477ef1f",
   "metadata": {},
   "source": [
    "#### Model 7: Building the model after adding July."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb3c31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding july variable and building the model.\n",
    "X_train_rfe['jul']=X_train['jul']\n",
    "\n",
    "# Adding a constant to X_train_rfe\n",
    "X_train_rfe=sm.add_constant(X_train_rfe)\n",
    "\n",
    "# Creating a LR object which we will use to fit the line.\n",
    "lr7 = sm.OLS(y_train, X_train_rfe).fit()\n",
    "\n",
    "#Checking the summary\n",
    "lr7.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae89592d",
   "metadata": {},
   "source": [
    "### The p-value increased after adding july. We should drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a511801b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rfe.drop(columns='jul', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c10396",
   "metadata": {},
   "source": [
    "#### Model 8: Building the model after adding wednesday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c875e609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding jan variable and building the model.\n",
    "X_train_rfe['wed']=X_train['wed']\n",
    "\n",
    "# Adding a constant to X_train_rfe\n",
    "X_train_rfe=sm.add_constant(X_train_rfe)\n",
    "\n",
    "# Creating a LR object which we will use to fit the line.\n",
    "lr8 = sm.OLS(y_train, X_train_rfe).fit()\n",
    "\n",
    "#Checking the summary\n",
    "lr8.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1938f5ed",
   "metadata": {},
   "source": [
    "### The p-value increased after adding wednesday. We should drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd056228",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rfe.drop(columns='wed', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0529684a",
   "metadata": {},
   "source": [
    "#### Model 9: Building the model after adding workingday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5641f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding workingday variable and building the model.\n",
    "X_train_rfe['workingday']=X_train['workingday']\n",
    "\n",
    "# Adding a constant to X_train_rfe\n",
    "X_train_rfe=sm.add_constant(X_train_rfe)\n",
    "\n",
    "# Creating a LR object which we will use to fit the line.\n",
    "lr9 = sm.OLS(y_train, X_train_rfe).fit()\n",
    "\n",
    "#Checking the summary\n",
    "lr9.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91286cec",
   "metadata": {},
   "source": [
    "#### The p-value increased after adding working day We should drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3564294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rfe.drop(columns='workingday', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f58fd9",
   "metadata": {},
   "source": [
    "#### Model 10: Building the model after adding thursday ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3e6bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding workingday variable and building the model.\n",
    "X_train_rfe['thurs']=X_train['thurs']\n",
    "\n",
    "# Adding a constant to X_train_rfe\n",
    "X_train_rfe=sm.add_constant(X_train_rfe)\n",
    "\n",
    "# Creating a LR object which we will use to fit the line.\n",
    "lr10 = sm.OLS(y_train, X_train_rfe).fit()\n",
    "\n",
    "#Checking the summary\n",
    "lr10.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a333d27",
   "metadata": {},
   "source": [
    "### The p-value increased after adding thursday. We should drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d65e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rfe.drop(columns='thurs', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a98864f",
   "metadata": {},
   "source": [
    "#### Observation: After evaluating various models, we can consider Model 3 as best fit with adjusted R square value ~84%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c0628d",
   "metadata": {},
   "source": [
    "#### Recalculating VIF for model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc7229f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recalculating VIF\n",
    "X_train_rfe = X_train_rfe.drop(['const'], axis=1) # Dropping constant variable from the df\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "X = X_train_rfe\n",
    "vif['Features'] = X.columns\n",
    "vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628f5cb6",
   "metadata": {},
   "source": [
    "#### We have VIFs of all feature variables below 5, so there is no multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9125b2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the feature variables used.\n",
    "\n",
    "X_train_rfe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b635226b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the constant\n",
    "\n",
    "X_train_rfe = sm.add_constant(X_train_rfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da2f4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the model\n",
    "\n",
    "lm = sm.OLS(y_train,X_train_rfe).fit()   # Running the linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a68c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the summary\n",
    "\n",
    "lm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a40e8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = lm.predict(X_train_rfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc70a54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the parameters obtained\n",
    "lr3.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38216b6e",
   "metadata": {},
   "source": [
    "#### Observation: This model looks good, as there seems to be VERY LOW Multicollinearity between the predictors and the p-values for all the predictors seems to be significant. For now, we will consider this as our final model (unless the Test data metrics are not significantly close to this number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a117a92",
   "metadata": {},
   "source": [
    "#### Step 8: Final Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e1cc4a",
   "metadata": {},
   "source": [
    "#### Hypothesis Testing :\n",
    "#### Hypothesis Testing States that\n",
    "#### H0:B1=B2=...=Bn=0\n",
    "#### H1: at least one Bi!=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa35825f",
   "metadata": {},
   "source": [
    "#### Observation: From the lr3 model summary, it is evident that all our coefficients are not equal to zero, which means we REJECT the NULL HYPOTHESIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcb974f",
   "metadata": {},
   "source": [
    "#### Model Validation: Validating Linear Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0279038",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.graphics.plot_ccpr(lr3, 'temp')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35831cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.graphics.plot_ccpr(lr3, 'windspeed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d9bd77",
   "metadata": {},
   "source": [
    "##### Observation: The above plots represents the relationship between the model and the predictor variables. \n",
    "##### As we can see, linearity is well preserved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59630090",
   "metadata": {},
   "source": [
    "#### Model Validation: Homoscedasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7d52c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = lr3.predict(X_train_lm3)\n",
    "residual = y_train - y_train_pred\n",
    "plt.title(\"Homoscedasticity\")\n",
    "plt.scatter(y_train,residual)\n",
    "plt.plot(y_train,(y_train - y_train), '-r')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Residual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77c8bf7",
   "metadata": {},
   "source": [
    "#### Observation: There is no visible pattern in residual values, thus homoscedacity is well preserved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c2d5f3",
   "metadata": {},
   "source": [
    "#### Model Validation: Heteroskedasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ad93a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Heteroskedasticity\")\n",
    "plt.scatter(y_train_pred, (y_train-y_train_pred))\n",
    "plt.xlabel(\"Fitted values\")\n",
    "plt.ylabel(\"Residuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154dec63",
   "metadata": {},
   "source": [
    "##### No Heteroskedasticity.\n",
    "##### From the scatter plot, we do not see a funnel like pattern and most of the points are centered around zero. So we do not have any heteroskedasticity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d886b6a",
   "metadata": {},
   "source": [
    "#### Model Validation: Independence of residuals\n",
    "##### Autocorrelation refers to the fact that observations’ errors are correlated. To verify that the observations are not auto-correlated, we can use the Durbin-Watson test. The test will output values between 0 and 4. The closer it is to 2, the less auto-correlation there is between the various variables.\n",
    "\n",
    "##### 0 – 2: positive auto-correlation\n",
    "##### 2 – 4: negative auto-correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3876157",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The Durbin-Watson value for Final Model lr 3 is',round(sm.stats.stattools.durbin_watson((y_train - y_train_pred)),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd204589",
   "metadata": {},
   "source": [
    "##### Observation: There is almost no autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd56639",
   "metadata": {},
   "source": [
    "##### Model Validation: Residuals must be normally distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d24924",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = y_train-y_train_pred\n",
    "\n",
    "# Plot the histogram of the error terms\n",
    "fig = plt.figure()\n",
    "sns.distplot((res), bins = 20)\n",
    "fig.suptitle('Error Terms')                  \n",
    "plt.xlabel('Errors')                         \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44dc74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.qqplot((y_train - y_train_pred), fit=True, line='45')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016be263",
   "metadata": {},
   "source": [
    "#### Observation : Based on the histogram, we can conclude that error terms are following a normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d55dd0",
   "metadata": {},
   "source": [
    "#### Step 9 : Making Predictions on Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f87d1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the variables to be scaled\n",
    "\n",
    "rescalevar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52539f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying scaling on test data\n",
    "\n",
    "df_test[rescalevar]=scaler.fit_transform(df_test[rescalevar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b57ad97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dda89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting target and feature variables\n",
    "\n",
    "y_test = df_test.pop('cnt')\n",
    "X_test = df_test[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcd8ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing feature variables\n",
    "\n",
    "X_train_rfe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bf9998",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping constant variable\n",
    "\n",
    "X_train_rfe.drop(columns='const', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c0926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's use our model to make predictions.\n",
    "\n",
    "# Creating X_test_new dataframe by dropping variables from X_test\n",
    "X_test_new = X_test[X_train_rfe.columns]\n",
    "\n",
    "# Adding a constant variable \n",
    "X_test_new = sm.add_constant(X_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08176e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "\n",
    "y_pred = lm.predict(X_test_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a98c117",
   "metadata": {},
   "source": [
    "#### Step 10: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143c37d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting y_test and y_pred to understand the spread.\n",
    "fig = plt.figure()\n",
    "plt.scatter(y_test,y_pred)\n",
    "fig.suptitle('y_test vs y_pred', fontsize=20)              # Plot heading \n",
    "plt.xlabel('y_test', fontsize=18)                          # X-label\n",
    "plt.ylabel('y_pred', fontsize=18)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3edf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35793d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(mean_squared_error(y_true=y_train, y_pred=y_train_pred))\n",
    "print(mean_squared_error(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3d3c9a",
   "metadata": {},
   "source": [
    "##### We have a Mean Squared Error close to 0 on the training dataset, meaning our model is able to correctly predict all variances in the data.\n",
    "##### On the test validation dataset, MSE is 0.01 also close to zero, meaning our model is able perform similarly on unknown data sets too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a99461",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r2_score(y_true=y_train, y_pred=y_train_pred))\n",
    "print(r2_score(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c226e6cd",
   "metadata": {},
   "source": [
    "#### We have a R-squared value of 84.26 % on train data and 81.48% on test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef00f03",
   "metadata": {},
   "source": [
    "### Conclusions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c44bd5",
   "metadata": {},
   "source": [
    "##### 1. We had a dataset with 510 records of data with the target and feature variables.\n",
    "##### 2. Performed EDA on the dataset to check for significant inferences and identify variables for data preparation. Used scatter plots for numerical and boxplots for categorical variables.\n",
    "##### 3. Prepared the data by converting categorical variables into dummy variables.\n",
    "#####  - season\n",
    "#####  - weathersit\n",
    "#####  - month\n",
    "#####  - weekday\n",
    "#####  - registered\n",
    "##### 4. Dropped irrelevant and categorical variables from the data.\n",
    "##### - season\n",
    "##### - weathersit\n",
    "##### - month\n",
    "##### - weekday\n",
    "##### - instant\n",
    "##### - dteday\n",
    "##### - atemp\n",
    "##### - registered\n",
    "##### - casual\n",
    "##### 5. Split the data into test and train datasets in a 70:30 ratio.\n",
    "###### 6. Rescaled numerical variables using MinMax method.\n",
    "###### 7.Plotted a heatmap to check linearity among all the variables and identified temp to be the most significant feature.\n",
    "###### 8.Built a model using only temp feature with 41% adjusted R-Square.\n",
    "###### 9.Adopted RFE for feature selection and built 5 other models to increase adjusted R-square to 84.3%.\n",
    "##### 10. Manually identified other feature variables and built 6 other models and observed most of newly added features are having p-value > 0.05 which is insignificant.\n",
    "##### 11.Performed residual analysis to confirm assumption of residuals hold true.\n",
    "##### 12.Made predictions on the train data.\n",
    "##### 13. Evaluated the model on test data with 81.5 % accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
